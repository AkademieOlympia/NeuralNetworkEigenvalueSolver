{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkEigenvalueSolver",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuYz3PN1FlI4",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Eigenvalue Solver With Tensorflow\n",
        "\n",
        "This notebook is an implementation of the algorithm presented in the paper [_Neural Networks Based Approach for Computing Eigenvectors and Eigenvalues of Symmetric Matrix_](https://www.sciencedirect.com/science/article/pii/S0898122104901101) by Zhang Yi and Yan Fu.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Below is an algorithm for a neural network that can be used to find the smallest or largest eigenvectors of a symmetric, positive definite matrix.\n",
        "\n",
        "* Positive definite: z$^T$Az > 0 for ever nonzero column, z, of A\n",
        "* Symmetric: A$^T$ = A\n",
        "\n",
        "Eigenvectors, and the resulting eigenvalues, of matrices are not only used in mathematics, but are important in many science and engineering applications.  Particularly in physics it is important to know the eigenvectors and eigenvalues of matrices known as Hamiltonains, which describe a physical system.  The smallest eigenvalue of a Hamiltonian is the lowest energy of the system and the largest eigenvalue is the highest energy.  The eigenvectors that correspond to the smallest and largest eigenvalues represent the system in its lowest and highest energy states.  Typically in physics, the lowest and highest eigenvalues, and corresponding eigenvectors, are the most important.\n",
        "\n",
        "## Neural Network Architecture\n",
        "\n",
        "The neural network calculates an eigenvector of a given matrix using the following differential equation.  Let A be an nxn symmetric matrix, x(t) be the trial eigenvector (i.e. the output of the neural network), and I is the nxn identity matrix.\n",
        "\n",
        "$\\frac{dx(t)}{dt} = \\dot{x}(t) = -x(t) + f(x(t))$\n",
        "\n",
        "for t $\\geq$ 0, where $f(x) = [x^TxA + (1-x^TAx)I]x$.  \n",
        "\n",
        "When x(t), the output of the neural network, is fully converged (i.e. is no longer changing) then $\\dot{x}(t) = 0$.  Then\n",
        "\n",
        "$ 0 = -x(t) + f(x(t))$\n",
        "\n",
        "$ x(t) = f(x(t))$\n",
        "\n",
        "Therefore, the neural network is converged with its output (the trial eigenvector), x, is equal to the value of f(x).  The loss function of the neural network is defined to be MSE (x, f(x)), where MSE is the mean-squared error function.\n",
        "\n",
        "$MSE(x, y) = \\sum_i (x_i - y_i)^2$\n",
        "\n",
        "In practice the beginning trial eigenvector, x(0), will have to be a vector of random numbers of the correct dimension.  However, it can be shown that any guess for the eigenvector will cause the neural network to converge to a true eigenvector of the matrix (See Theorem 3 of the above paper).\n",
        "\n",
        "Once the neural network has determined the converged eigenvector, x(t $\\rightarrow\\infty$), the eigenvalue of the matrix corresponding to that eigenvector can be found using the below equation, which is known as the Rayleigh quotient.\n",
        "\n",
        "$\\lambda (t \\rightarrow\\infty) = \\frac{x(t \\rightarrow\\infty)^TAx(t \\rightarrow\\infty)}{x(t \\rightarrow\\infty)^Tx(t \\rightarrow\\infty)}$\n",
        "\n",
        "## A Deviation from the Paper's Algorithm\n",
        "\n",
        "Though the author's of the paper suggest that there is a predetermined way to guarentee the algorithm searches for the smallest or largest eigenvalues, this is not found to work in practice.  Instead, to ensure that the smallest or the largest eigenvalues can always be found a change was made to the loss function, which will be described below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njkoGNb5qt2j",
        "colab_type": "text"
      },
      "source": [
        "## Code\n",
        "\n",
        "\n",
        "Below is the code for implenting the above algorithm.\n",
        "\n",
        "Note: this code is written using Tensorflow 1.  To run the code with Tensorflow 2 replace the `import tensorflow as tf` line with \n",
        "\n",
        "```\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4zK-WiMF3vS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "64bcae8b-10e1-46a4-91b8-dae8716668f6"
      },
      "source": [
        "##############################\n",
        "# IMPORTS\n",
        "##############################\n",
        "# Third-Party Imports\n",
        "# For machine learning, making sure to import version 1 of Tensorflow, not \n",
        "# version 2\n",
        "import tensorflow as tf\n",
        "# For matrix manipulation and finding analytical eigenvalues\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQOymsVrptfO",
        "colab_type": "text"
      },
      "source": [
        "A method for the function f(x), defined above.  This will be used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oKIyy6ypd_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################\n",
        "# F(X)\n",
        "##############################\n",
        "def f_x(x, A):\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            x (a Tensorflow tensor): the trial eigenvector (i.e. the output\n",
        "                of the neural network)\n",
        "            A (a 2D Numpy array): the matrix to find eigenvectors of\n",
        "        Returns:\n",
        "            f (a Tensorflow tensor): the result of the function f(x(t)), \n",
        "                defined in the paper referenced above.  When x(t) is \n",
        "                converged, f(x(t)) = x(t)\n",
        "        Returns the value of f(x) at a given value of x.\n",
        "    \"\"\"\n",
        "    xTxA = (tf.tensordot(tf.transpose(x), x, axes=1)*A)\n",
        "    # (1- xTAx)*I\n",
        "    xTAxI = (1- tf.tensordot(tf.transpose(x), tf.tensordot(A, x, axes=1), axes=1))*np.eye(matrix_size)\n",
        "    # (xTx*A - (1- xTAx)*I)*x\n",
        "    f = tf.tensordot((xTxA + xTAxI), x, axes=1)\n",
        "\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94G7HZG1p5oy",
        "colab_type": "text"
      },
      "source": [
        "The code for the neural network.  The code is split into five sections:\n",
        "1. Set-up.  The first section of code defines the identity matrix, I, and the initial trial eigenvector, x0.  These must then be converted to tensors in order to be used with Tensorflow.\n",
        "2. Next the neural network is set up using the argument nn_structure, a list of numbers.  The length of the list indicates how many hidden layers are in the network design and each number in the list indicates the number of hidden neurons in that layer.  A final output layer is added to the end of the hidden layers, with its dimension being the size of the eigenvector.\n",
        "3. The loss function is defined to be the mean squared error between x, the output of the neural network, and f(x), with an additional term added to control which eigenvector is found.  The argument eigen_guess controls which eigenvector is found.  A guess larger than the largest eigenvalue of the matrix will always result in the largest eigenvector being found a a guess smaller then the smallest eigenvalue will always result in the smallest eigenvector being found.  In practice when the approximate eigenvalues are not known, the guesses can be substituted with a very large number with a positive sign for the largest eigenvector and a negative sign for the smallest eigenvector.  The argument eigen_lr, controls how much this term contributes to the overall loss function.  Setting this term to zero means that the neural network will find any eigenvector of the given matrix.\n",
        "4. Setting up the training portion of the neural network by defining the optimizer and instructing the neural network to minimize the loss fuction.\n",
        "5. For each training iteration of the neural network the trial eigenvalue is calculated from the trial eigenvector.  The change of this eigenvalue from the last eigenvalue is calculaed to end the training process with sequential eigenvalues are sufficiently close.  For every hundreth training iteration the current state of the network is displayed to the console, if verbose is set to `True` in the arguments.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nReahOI8SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################\n",
        "# NN EIGENVALUE\n",
        "##############################\n",
        "def NN_Eigenvalue(matrix_size, A, max_iterations, nn_structure, eigen_guess, \n",
        "                    eigen_lr, delta_threshold, verbose):\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            matrix_size (an int): the dimension of the matrix\n",
        "            A (a 2D Numpy array): A square, symmetric matrix to find \n",
        "                an eigenvector and eigenvalue of.\n",
        "            max_iterations (an int): the maximum number of training iterations \n",
        "                to be used by the neural network\n",
        "            nn_structure (a list): the number of neurons in each layer of the\n",
        "                neural network\n",
        "            eigen_guess (an int): to find the lowest eigenvalue, a number smaller\n",
        "                than the predicted eigenvalue.  To find the largest eigenvalue,\n",
        "                a number larger than the predicted eigenvalue.\n",
        "            eigen_lr (a float): the learning rate for the portion of the loss\n",
        "                function that controls which eigenvalue is found.  Set to 0.0\n",
        "                to find a random eigenvalue.\n",
        "            delta_threshold (a float): the minimum value desired between two\n",
        "                sequentially calculated eigenvalues\n",
        "            verbose (a boolean): True case prints the state of the eigenvalue for\n",
        "                every 100th training iteration.\n",
        "        Returns:\n",
        "            eigenvalue (a float): the predicted eigenvalue of matrix A\n",
        "            eigenvector (a numpy array): the predicted eigenvector of the matrix \n",
        "                A\n",
        "        Computes a prediction for an eigenvalue and an eigenvector of a given\n",
        "        matrix using a neural network.  Parameters are given to control which\n",
        "        eigenvalue and eigenvector are found.\n",
        "    \"\"\"\n",
        "    # Defining the 6x6 identity matrix\n",
        "    I = np.identity(matrix_size)\n",
        "    \n",
        "    # Create a vector of random numbers and then normalize it\n",
        "    # This is the beginning trial solution eigenvector\n",
        "    x0 = np.random.rand(matrix_size)\n",
        "    x0 = x0/np.sqrt(np.sum(x0*x0))\n",
        "    # Reshape the trial eigenvector into the format for Tensorflow\n",
        "    x0 = np.reshape(x0, (1, matrix_size))\n",
        "\n",
        "    # Convert the above matrix and vector into tensors that can be used by\n",
        "    # Tensorflow\n",
        "    I_tf = tf.convert_to_tensor(I)\n",
        "    x0_tf = tf.convert_to_tensor(x0, dtype=tf.float64)\n",
        "\n",
        "    # Set up the neural network with the specified architecture\n",
        "    with tf.variable_scope('dnn'):\n",
        "        num_hidden_layers = np.size(nn_structure)\n",
        "\n",
        "        # x0 is the input to the neural network\n",
        "        previous_layer = x0_tf\n",
        "        # Hidden layers\n",
        "        for l in range(num_hidden_layers):\n",
        "            current_layer = tf.layers.dense(previous_layer, nn_structure[l],activation=tf.nn.relu )\n",
        "            previous_layer = current_layer\n",
        "\n",
        "        # Output layer\n",
        "        dnn_output = tf.layers.dense(previous_layer, matrix_size)\n",
        "      \n",
        "    # Define the loss function\n",
        "    with tf.name_scope('loss'):\n",
        "        # trial eigenvector is the output of the neural network\n",
        "        x_trial = tf.transpose(dnn_output) \n",
        "        # f(x)\n",
        "        f_trial = tf.transpose(f_x(x_trial, A))\n",
        "        # eigenvalue calculated using the trial eigenvector using the \n",
        "        # Rayleigh quotient formula\n",
        "        eigenvalue_trial = tf.transpose(x_trial)@A@x_trial/(tf.transpose(x_trial)@x_trial)\n",
        "        \n",
        "        x_trial = tf.transpose(x_trial) \n",
        "\n",
        "        # Define the loss function\n",
        "        loss = tf.losses.mean_squared_error(f_trial, x_trial) + \\\n",
        "                eigen_lr*tf.losses.mean_squared_error([[eigen_guess]], eigenvalue_trial)\n",
        "                                                                                                        \n",
        "    # Define the training algorithm and loss function\n",
        "    with tf.name_scope('train'):\n",
        "        optimizer = tf.train.AdamOptimizer()\n",
        "        training_op = optimizer.minimize(loss)\n",
        "\n",
        "    ## Execute the Tensorflow session\n",
        "    with tf.Session() as sess:  \n",
        "        # Initialize the Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        init.run()\n",
        "\n",
        "        # Define for calculating the change between successively calculated\n",
        "        # eigenvalues\n",
        "        old_eig = 0\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            sess.run(training_op)\n",
        "            # Compute the eigenvalue using the Rayleigh quotient\n",
        "            eigenvalue = (x_trial.eval() @ (A @ x_trial.eval().T)\n",
        "                        /(x_trial.eval() @ x_trial.eval().T))[0,0]\n",
        "            eigenvector = x_trial.eval()\n",
        "\n",
        "            # Calculate the change between the currently calculated eigenvalue\n",
        "            # and the previous one\n",
        "            delta = np.abs(eigenvalue-old_eig)\n",
        "            old_eig = eigenvalue\n",
        "            \n",
        "            # Print useful information every 100 steps\n",
        "            if verbose:\n",
        "                if i % 100 == 0:\n",
        "                    l = loss.eval()\n",
        "                    print(\"Step:\", i, \"/\",max_iterations, \"loss: \", l,\n",
        "                          \"Eigenvalue:\" , eigenvalue)\n",
        "            # Kill the loop if the change in eigenvalues is less than the \n",
        "            # given threshold\n",
        "            if delta < delta_threshold: \n",
        "                break\n",
        "\n",
        "    # Return the converged eigenvalue and eigenvector\n",
        "    return eigenvalue, eigenvector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02uXtXxrrsZx",
        "colab_type": "text"
      },
      "source": [
        "## Test Matrices\n",
        "\n",
        "The above neural network will work with any symmetric, positive definite matrix.  A matrix, A, can be made symmetric using the following formula, $A_{sym} = (A + A^T)/2$.  A symmetric matrix filled with random numbers can be created using the following code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBRyRtxmsN8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_symmetric (matrix_size):\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            matrix_size (an int): the size of the matrix to be constructed\n",
        "        Returns:\n",
        "            A (a 2D Numpy array): a symmetric matrix\n",
        "        Constructs a symmetric matrix of the given size filled with random numbers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a matrix filled with random numbers\n",
        "    A = np.random.rand (matrix_size, matrix_size)\n",
        "\n",
        "    # Ensure that matrix A is symmetric\n",
        "    A = (np.transpose(A) + A) / 2\n",
        "\n",
        "    return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7NnMNOqtVOW",
        "colab_type": "text"
      },
      "source": [
        "Symmetric matrices can also occur with physical relevance.  The following sets up the Hamiltonian for the 4 particle 4 hole pairing model with no broken pairs.  More information on the pairing model can be found in Chapter 10 of  [this book](https://link.springer.com/book/10.1007/978-3-319-53336-0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMYEsnuIh-rT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairing_model_4p4h (g, delta):\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            g (a float): the interaction strength\n",
        "            delta (a float): the spacing between energy levels\n",
        "        Returns:\n",
        "            A (a 2D Numpy array): the Hamiltoian for the 4 particle, 4 hole\n",
        "                pairing model\n",
        "        Calculates the Hamiltonian for the 4 particle 4 hole pairing model for\n",
        "        the no broken pairs case.  For more information see Chapter 10 of\n",
        "        LNP 936.\n",
        "    \"\"\"\n",
        "    A = np.array(\n",
        "        [[2*delta-g,    -0.5*g,     -0.5*g,     -0.5*g,    -0.5*g,          0.],\n",
        "        [   -0.5*g, 4*delta-g,     -0.5*g,     -0.5*g,        0.,     -0.5*g ], \n",
        "        [   -0.5*g,    -0.5*g,  6*delta-g,         0.,    -0.5*g,     -0.5*g ], \n",
        "        [   -0.5*g,    -0.5*g,         0.,  6*delta-g,    -0.5*g,     -0.5*g ], \n",
        "        [   -0.5*g,        0.,     -0.5*g,     -0.5*g, 8*delta-g,     -0.5*g ], \n",
        "        [       0.,    -0.5*g,     -0.5*g,     -0.5*g,    -0.5*g, 10*delta-g ]])\n",
        "    return A\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUwPw5v2vjuF",
        "colab_type": "text"
      },
      "source": [
        "## Test the Accuracy of the Neural Network\n",
        "\n",
        "The below code uses the neural network to find the eigenvalue of a given matrix.  It then compares the neural network eigenvalue to the analytical eigenvalue calcualted with the Numpy library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKiw2ujlplPW",
        "colab_type": "code",
        "outputId": "67799818-7b44-49de-dedf-36fb8b3fad36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Defining variables\n",
        "matrix_size = 6 # Size of the matrix\n",
        "max_iterations = 5000 # Maximum number of iterations\n",
        "nn_structure = [100,100] # Number of hidden neurons in each layer\n",
        "eigen_guess =  70 # Guess for the eigenvalue (see the header of NN_Eigenvalue)\n",
        "eigen_lr = 0.01 # Eigenvalue learnign rate (see the header of NN_Eigenvalue)\n",
        "delta_threshold = 1e-16 # Kill condition\n",
        "verbose = True # True to display state of neural network evrey 100th iteration\n",
        "\n",
        "# Create the matrix to be used\n",
        "A = random_symmetric (matrix_size)\n",
        "#A = pairing_model_4p4h (0.5, 1.0)\n",
        "\n",
        "# Find the eigenvalues and the eigenvectors using Numpy to compare to the \n",
        "numpy_eigenvalues, numpy_eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "# Reset the Tensorflow graph, causes an error if this is not here\n",
        "# Since the above cells are not re-ran every time this one is, they are not \n",
        "# reset.  This line is needed to reset the Tensorflow computational graph to\n",
        "# avoid variable redefinition errors. \n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Calcualte the estimate of the eigenvalue and the eigenvector\n",
        "eigenvalue, eigenvector = NN_Eigenvalue(matrix_size, A, max_iterations,\n",
        "                                        nn_structure, eigen_guess, eigen_lr, \n",
        "                                        delta_threshold, verbose)\n",
        "\n",
        "## Compare with the analytical solution\n",
        "print(\"\\n Numpy Eigenvalues: \\n\", numpy_eigenvalues)\n",
        "print(\"\\n Final Numerical Eigenvalue \\n\", eigenvalue)\n",
        "diff = np.min(abs(numpy_eigenvalues - eigenvalue))\n",
        "print(\"\\n\")\n",
        "print('Absolute difference between Numerical Eigenvalue and TensorFlow DNN = ',diff)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-0bcfb81a6960>:53: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Step: 0 / 5000 loss:  48.320076 Eigenvalue: 0.48735753924919706\n",
            "Step: 100 / 5000 loss:  44.71716 Eigenvalue: 3.129111427296011\n",
            "Step: 200 / 5000 loss:  44.717133 Eigenvalue: 3.129124544393247\n",
            "\n",
            " Numpy Eigenvalues: \n",
            " [ 3.12912454 -0.80240881 -0.29930002  0.03618002  0.3708515   0.60005076]\n",
            "\n",
            " Final Numerical Eigenvalue \n",
            " 3.129124544563733\n",
            "\n",
            "\n",
            "Absolute difference between Numerical Eigenvalue and TensorFlow DNN =  7.682743330406083e-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aEyrh2O5o0-",
        "colab_type": "text"
      },
      "source": [
        "## Test to See How Many Eigenvalues the Network Can Find\n",
        "\n",
        "This test will run the program many times, without using an eigenvalue guess, to see if the network is capable of finding all of the eigenvalues, and in what distribution.  The neural network will be minimized quickest at whichever eigenvector of the matrix is closest to the initial trial eigenvector.  Since the initial trial eigenvector is initialized to a random vector, it may take several trials to find all of the eigenvalues, and even more to determine the probability distribution of finding them.\n",
        "\n",
        "The below code is runs the  neural network a set number of times and keeps track of which eigenvalue is found.  It then plots the results.\n",
        "\n",
        "Note that the neural network in this test converges significantly slower than the neural network in the above test due to the less constrained loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMQu81_h5pFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "113018ee-8742-4134-d2a7-0981088be981"
      },
      "source": [
        "# Defining variables\n",
        "matrix_size = 6 # Size of the matrix\n",
        "max_iterations = 50000 # Maximum number of iterations\n",
        "nn_structure = [100] # Number of hidden neurons in each layer\n",
        "eigen_guess =  0.0 # No guess for the eigenvalue\n",
        "eigen_lr = 0.0 # Set the eigenvalue learning rate to zero to find all eigenvalues\n",
        "delta_threshold = 1e-16 # Kill condition\n",
        "verbose = False # No need to display state of neural network\n",
        "\n",
        "# Create the matrix to be used\n",
        "#A = random_symmetric (matrix_size)\n",
        "A = pairing_model_4p4h (0.5, 1.0)\n",
        "\n",
        "# Find the eigenvalues and the eigenvectors using Numpy to compare to the \n",
        "numpy_eigenvalues, numpy_eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "# Create a dictionary where the keys are the analytical eigenvalues of the matrix\n",
        "eigens_found_dic = {}\n",
        "for eigenval in numpy_eigenvalues:\n",
        "    # Round the keys to four decimal places to account for differences in the\n",
        "    # analytical and neural network solutions.  This number can be changed as \n",
        "    # needed.\n",
        "    eigens_found_dic[np.round(eigenval, decimals=4)] = 0\n",
        "\n",
        "# Number of different times to run the neural network\n",
        "trials = 10\n",
        "\n",
        "for i in range (trials):\n",
        "    print ('Trial ', i)\n",
        "    # Reset the Tensorflow graph \n",
        "    tf.reset_default_graph()\n",
        "    # Calcualte the estimate of the eigenvalue and the eigenvector\n",
        "    eigenvalue, eigenvector = NN_Eigenvalue(matrix_size, A, max_iterations, nn_structure, eigen_guess, \n",
        "                        eigen_lr, delta_threshold, verbose)\n",
        "    # Increment the correct entry in the dictionary\n",
        "    eigens_found_dic[np.round(eigenvalue, decimals=4)] += 1\n",
        "\n",
        "# Import matplotlib to make a graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the values stored in the dictionary then set the ticks on the x-axis\n",
        "# equal to the dictionary keys\n",
        "plt.bar(range(len(eigens_found_dic)), list(eigens_found_dic.values()), align='center')\n",
        "plt.xticks(range(len(eigens_found_dic)), list(eigens_found_dic.keys()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial  0\n",
            "Trial  1\n",
            "Trial  2\n",
            "Trial  3\n",
            "Trial  4\n",
            "Trial  5\n",
            "Trial  6\n",
            "Trial  7\n",
            "Trial  8\n",
            "Trial  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f5213364a90>,\n",
              "  <matplotlib.axis.XTick at 0x7f5213364320>,\n",
              "  <matplotlib.axis.XTick at 0x7f5213364128>,\n",
              "  <matplotlib.axis.XTick at 0x7f5213330ba8>,\n",
              "  <matplotlib.axis.XTick at 0x7f5213342128>],\n",
              " <a list of 5 Text xticklabel objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATtklEQVR4nO3df5BlZZ3f8feHYUBXEKqYdmXnh20F\n4kaNIvaOWEZDYElADGxWNjvU7qoU1mRdWTFxs4KpwhJrq/CP6JZipCZABBcFg64Z+aGSgFGrlsFm\ndviNm4nBzBB2GUBAomIGv/njntHrtbvv7Z7b3TPPvF9Vt/qc8zzn3u/pvvfT5z73nHtSVUiS9n8H\nLXcBkqTxMNAlqREGuiQ1wkCXpEYY6JLUiIOX64FXrVpVk5OTy/XwkrRfuvPOOx+rqomZ2pYt0Ccn\nJ5menl6uh5ek/VKS783W5pCLJDXCQJekRhjoktQIA12SGmGgS1IjDHRJasTIgZ5kRZK/TnLDDG2H\nJrkuyfYkW5JMjrNISdJw89lDPx94YJa2c4HvV9UxwMeAj+xtYZKk+Rkp0JOsAU4HLp+ly5nAVd30\n9cDJSbL35UmSRjXqmaJ/DvwpcPgs7auBHQBVtTvJU8BRwGP9nZJsBDYCrFu3biH16gA3ecGNy13C\n2Dx0yenLXYIaM3QPPclbgEer6s69fbCq2lRVU1U1NTEx41cRSJIWaJQhlzcAZyR5CLgWOCnJXwz0\neRhYC5DkYOAI4PEx1ilJGmJooFfVhVW1pqomgQ3ArVX1+wPdNgNv76bP6vp4sVJJWkIL/rbFJBcD\n01W1GbgC+EyS7cAT9IJfkrSE5hXoVfV14Ovd9EV9y38M/M44C5MkzY9nikpSIwx0SWqEgS5JjTDQ\nJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12S\nGjHKRaKfl+SOJHcluS/Jh2bo844ku5Js627vXJxyJUmzGeWKRc8CJ1XVM0lWAt9KcnNV3T7Q77qq\nOm/8JUqSRjE00LuLPT/Tza7sbl4AWpL2MSONoSdZkWQb8ChwS1VtmaHbW5PcneT6JGvHWqUkaaiR\nAr2qnquq44A1wPokrxzo8mVgsqpeBdwCXDXT/STZmGQ6yfSuXbv2pm5J0oB5HeVSVU8CtwGnDix/\nvKqe7WYvB147y/qbqmqqqqYmJiYWUq8kaRajHOUykeTIbvr5wCnAgwN9ju6bPQN4YJxFSpKGG+Uo\nl6OBq5KsoPcP4PNVdUOSi4HpqtoMvCfJGcBu4AngHYtVsCRpZqMc5XI38JoZll/UN30hcOF4S5Mk\nzYdnikpSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWp\nEQa6JDXCQJekRhjoktQIA12SGmGgS1IjRrmm6POS3JHkriT3JfnQDH0OTXJdku1JtiSZXIxiJUmz\nG2UP/VngpKp6NXAccGqSEwb6nAt8v6qOAT4GfGS8ZUqShhka6NXzTDe7srvVQLczgau66euBk5Nk\nbFVKkoYaepFogCQrgDuBY4BPVtWWgS6rgR0AVbU7yVPAUcBjA/ezEdgIsG7dugUXPXnBjQted1/z\n0CWnL3cJkhox0oeiVfVcVR0HrAHWJ3nlQh6sqjZV1VRVTU1MTCzkLiRJs5jXUS5V9SRwG3DqQNPD\nwFqAJAcDRwCPj6NASdJoRjnKZSLJkd3084FTgAcHum0G3t5NnwXcWlWD4+ySpEU0yhj60cBV3Tj6\nQcDnq+qGJBcD01W1GbgC+EyS7cATwIZFq1iSNKOhgV5VdwOvmWH5RX3TPwZ+Z7ylSZLmwzNFJakR\nBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGg\nS1IjDHRJaoSBLkmNMNAlqRGjXFN0bZLbktyf5L4k58/Q58QkTyXZ1t0umum+JEmLZ5Rriu4G3ldV\nW5McDtyZ5Jaqun+g3zer6i3jL1GSNIqhe+hV9UhVbe2mfwA8AKxe7MIkSfMzrzH0JJP0Lhi9ZYbm\n1ye5K8nNSV4xy/obk0wnmd61a9e8i5UkzW7kQE9yGPAF4L1V9fRA81bgJVX1auATwJdmuo+q2lRV\nU1U1NTExsdCaJUkzGCnQk6ykF+bXVNUXB9ur6umqeqabvglYmWTVWCuVJM1plKNcAlwBPFBVH52l\nz4u7fiRZ393v4+MsVJI0t1GOcnkD8AfAPUm2dcs+AKwDqKrLgLOAdyXZDfwI2FBVtQj1SpJmMTTQ\nq+pbQIb0uRS4dFxFSZLmzzNFJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWp\nEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRGjXFN0bZLbktyf5L4k58/QJ0k+\nnmR7kruTHL845UqSZjPKNUV3A++rqq1JDgfuTHJLVd3f1+c04Nju9jrgU91PSdISGbqHXlWPVNXW\nbvoHwAPA6oFuZwJXV8/twJFJjh57tZKkWY2yh/4zSSaB1wBbBppWAzv65nd2yx4ZWH8jsBFg3bp1\n86tU0gFt8oIbl7uEsXnoktMX5X5H/lA0yWHAF4D3VtXTC3mwqtpUVVNVNTUxMbGQu5AkzWKkQE+y\nkl6YX1NVX5yhy8PA2r75Nd0ySdISGeUolwBXAA9U1Udn6bYZeFt3tMsJwFNV9cgsfSVJi2CUMfQ3\nAH8A3JNkW7fsA8A6gKq6DLgJeDOwHfghcM74S5UkzWVooFfVt4AM6VPAu8dVlCRp/jxTVJIaYaBL\nUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1\nwkCXpEYY6JLUCANdkhoxyjVFr0zyaJJ7Z2k/MclTSbZ1t4vGX6YkaZhRrin6aeBS4Oo5+nyzqt4y\nlookSQsydA+9qr4BPLEEtUiS9sK4xtBfn+SuJDcnecVsnZJsTDKdZHrXrl1jemhJEown0LcCL6mq\nVwOfAL40W8eq2lRVU1U1NTExMYaHliTtsdeBXlVPV9Uz3fRNwMokq/a6MknSvOx1oCd5cZJ00+u7\n+3x8b+9XkjQ/Q49ySfI54ERgVZKdwAeBlQBVdRlwFvCuJLuBHwEbqqoWrWJJ0oyGBnpVnT2k/VJ6\nhzVKkpaRZ4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmN\nMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSI4YGepIrkzya5N5Z2pPk40m2J7k7yfHjL1OSNMwo\ne+ifBk6do/004NjuthH41N6XJUmar6GBXlXfAJ6Yo8uZwNXVcztwZJKjx1WgJGk0Qy8SPYLVwI6+\n+Z3dskcGOybZSG8vnnXr1o3hoQ9MkxfcuNwljM1Dl5y+3CVIzVjSD0WralNVTVXV1MTExFI+tCQ1\nbxyB/jCwtm9+TbdMkrSExhHom4G3dUe7nAA8VVW/NNwiSVpcQ8fQk3wOOBFYlWQn8EFgJUBVXQbc\nBLwZ2A78EDhnsYqVJM1uaKBX1dlD2gt499gqkiQtiGeKSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCX\npEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiNGCvQk\npyb5TpLtSS6Yof0dSXYl2dbd3jn+UiVJcxnlmqIrgE8CpwA7gW8n2VxV9w90va6qzluEGiVJIxhl\nD309sL2qvltVPwGuBc5c3LIkSfM1SqCvBnb0ze/slg16a5K7k1yfZO1Md5RkY5LpJNO7du1aQLmS\npNmM60PRLwOTVfUq4Bbgqpk6VdWmqpqqqqmJiYkxPbQkCUYL9IeB/j3uNd2yn6mqx6vq2W72cuC1\n4ylPkjSqUQL928CxSV6a5BBgA7C5v0OSo/tmzwAeGF+JkqRRDD3Kpap2JzkP+CqwAriyqu5LcjEw\nXVWbgfckOQPYDTwBvGMRa5YkzWBooANU1U3ATQPLLuqbvhC4cLylSZLmwzNFJakRBrokNcJAl6RG\nGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSB\nLkmNMNAlqREjBXqSU5N8J8n2JBfM0H5okuu69i1JJsddqCRpbkMDPckK4JPAacDLgbOTvHyg27nA\n96vqGOBjwEfGXagkaW6j7KGvB7ZX1Xer6ifAtcCZA33OBK7qpq8HTk6S8ZUpSRpmlItErwZ29M3v\nBF43W5+q2p3kKeAo4LH+Tkk2Ahu72WeSfGchRS+hVQxsw7hl330vs+jbDgf29h/I274P2x+e9y+Z\nrWGUQB+bqtoEbFrKx9wbSaaramq561gOB/K2w4G9/W77/rvtowy5PAys7Ztf0y2bsU+Sg4EjgMfH\nUaAkaTSjBPq3gWOTvDTJIcAGYPNAn83A27vps4Bbq6rGV6YkaZihQy7dmPh5wFeBFcCVVXVfkouB\n6araDFwBfCbJduAJeqHfgv1meGgRHMjbDgf29rvt+6m4Iy1JbfBMUUlqhIEuSY1oNtCTXJnk0ST3\nDun3G0l2Jzmrb9lXkjyZ5IaBvknyZ0n+JskDSd7TLT8iyZeT3JXkviTnLM5WjS7J85Lc0VfTh+bo\n+9YklWSqm/+9JNv6bj9NclzX9tok93Rf8/DxPSeQdV/9sKf/Q0m2Lc2WLlxX5z1dzdMztJ+Y5Km+\n7bpoOeqcjyQvG/jbPZ3kvQN95tyuJCuS/HX/8z/JSUm2Jrk3yVXd0Wz96/zS62i5JDm/q/O+wW3v\n2mfd/iRHJrk+yYPda/z13fIPJ7m76/+1JL/WLf/1JH+V5Nkkf7J0WzmLqmryBrwJOB64d44+K4Bb\ngZuAs/qWnwz8c+CGgf7nAFcDB3XzL+p+fgD4SDc9Qe+D4UOWefsDHNZNrwS2ACfM0O9w4BvA7cDU\nDO3/EPifffN3ACd0938zcNoM6/x74KLlfg6M8Dt6CFg1R/uJg8+B/enWPb//FnjJfLYL+DfAZ/f0\nobfjtwP4+938xcC5A4/zS6+jZdrmVwL3Ar9C76CP/wocM+r20zvj/Z3d9CHAkd30C/v6vAe4rJt+\nEfAbwJ8Bf7Lcf/Nm99Cr6hv0gnUufwx8AXh0YN3/Bvxghv7vAi6uqp92/fasV8Dh3d7qYd3j7l54\n9Xuvep7pZld2t5k+Af8wve/e+fEsd3U2va97IMnR9J7Yt1fv2Xw18Fv9nbvfwb8EPrfXG6G9dTK9\nf8bfG3WFJGuA04HL+xYfBfykqv6mm78FeGtf+4yvo2XyD4AtVfXDqtoN/Hfgt0dZMckR9HYErwCo\nqp9U1ZPd9NN9XV9A91qqqker6tvA/xvfJixcs4E+TJLVwL8APjWP1f4e8LtJppPcnOTYbvml9J5I\n/we4Bzh/T+gvp+6t8zZ6L7RbqmrLQPvxwNqqunGOu/ldfh7Oq+l99cMeO7tl/d4I/F1V/Y+9Kn5p\nFPC1JHem97UUM3l9N2x1c5JXLGVxY7CB2f+xzrZdfw78KdD//H0MOHjPkBy9c032nEi4kNfRYroX\neGOSo5L8CvBmfvHEyD1m2v6XAruA/9QNOV2e5AV7VuiGW3cAvwfsk8NvB2yg03vivn+ewXso8OPq\nnRr8H4Eru+X/DNgG/BpwHHBpkheOs9iFqKrnquo4emf3rk/yyj1tSQ4CPgq8b7b1k7wO+GFVzfk5\nxICz2X/2zv9RVR1P75tE353kTQPtW+kNV7wa+ATwpaUucKHSOwnwDOA/z9A843YleQvwaFXd2d+5\neze2AfhYkjvovXt9rmteyOto0VTVA/TecX4N+Aq91+VzA91m+7seTG+Y9lNV9Rrg/wI/+7rwqvp3\nVbUWuAY4bzG3Y6EO5ECfAq5N8hC9PY7/kOS35l6FncAXu+m/BF7VTZ8DfLEb5tgO/C/g18df8sJ0\nbxtvA07tW3w4vfHGr3e/gxOAzX17YfDLe3gP0/vnsMcvfA1E90HZbwPXjbP+xVJVD3c/H6X391w/\n0P70nmGrqroJWJlk1ZIXujCnAVur6u8GG+bYrjcAZ3TPh2uBk5L8Rdfvr6rqjVW1nt5nLnuGXxby\nOlpUVXVFVb22qt4EfJ+f17qnfbbt3wns7Hsnez29gB90Db845LTPOGADvapeWlWTVTVJ7w/3R1U1\nbA/sS8A/6ab/MT9/ovxveuOVJPlV4GXAd8de9DwkmUhyZDf9fOAU4ME97VX1VFWt6vsd3A6cUVXT\n3ToH0RsLv7ZvnUeAp5Oc0I2Vvw34L30P+5vAg1XVPyyzT0rygiSH75kG/im9t+v9fV7cdxTPenqv\nl/3lO4pmfac023ZV1YVVtaZ7Pmyg9xUev9/1e1H381Dg/cBlsODX0aLqq3UdvR2Mzw60z7b9fwvs\nSPKyruvJwP1dv2P77uJM+l5L+5Il/bbFpZTkc/Q+zV6VZCfwQXofDFJVlw1Z95v09rAP69Y9t6q+\nClwCXJPkXwPPAO/sVvkw8Okk99A7+uP9VbXcXz96NHBVehcoOQj4fFXdkF/8yoa5vAnYUVWD/5j+\nCPg08Hx6R7nc3Nc215jtvuZXgb/sXtcHA5+tqq8k+UP42XPkLOBdSXYDPwI2dMMP+7TuH9QpwL/q\nW7a32/VvuyGZg+gNSdy6KMWPxxeSHEXvg8p3V9WT89j+P6b3Gj+E3k7ZnkOQL+mC/qfA94A/hN4/\nB2AaeCHw0+4wyZcPfIi6ZDz1X5IaccAOuUhSawx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1Ij/\nD3EsOJTKk3APAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}